#####One crucial aspect of AI Agents is their ability to take actions. As we saw, this happens through the use of Tools.#####


<----Definition of tools design/integrate them into the agent via (SYSTEM-MESSAGE)----->
--------------------------------------------------------------------------------
NB:::By giving your Agent the right Tools‚Äîand clearly describing how those Tools work‚Äîyou can dramatically increase what your AI can accomplish. 








<----Commonly used tools in AI-Agents--->
--------------------------------------------------------------------------------
Web Search ---> Allows the agent to fetch up-to-dte information from the internet.

image Generation --->  Creates images based on text descriptions.

Retrieval ---> Retrieves information from external source.

API Interface ---> Interacts with an external PI (github,youtube,spotify etc.)

Definition of good tools: Something that complements the power of an LLM
REMEMBER: this tools are just exsamples as you can in fact create tools for any use case!
NB:::: LLms predict the completion of a prompt based on their training data, wich means that their internal knowledge/intelligence  only includes events prior too their training, therefor if you agents need up to date  data you need too provide it through some tool.
for exsample ---> if you needed to perform arithmetic giving a calculator tool to your LLM will provide better results than relying on the native capabilities of the model.
for exsample ---> if you ask an LLM directly (without the search tool) for today's weather the LLM will potentially hallucinate random weather.







<----- A tools should contain----->
---------------------------------------------------------------------------------
1. a Textual representation  of what the function does.
2. a Callable (something to perform an action)
3. Arguments with typing
4.(optional) outputs with typings.





<-----How does tools work?----->
---------------------------------------------------------------------------------
LLM's can only recieve text as input and generate text outputs, they have no way of calling tools on their own.
The way we provide tools to an agent is that we teach it about the existence of tools
Then we teach the model to generate text that invoke those tools when it needs to!
exsample---> if we provided the model with a tool too check the weather at a location from the internet and then ask the LLM about the weather in paris, the LLM will recognize that question as an relevant oppertuntity to use the ("Weather") tool we thought it about. 
The LLM will generate text in form of code to invoke that tool
it's the responibility of an agent to parse the LLM's output recognize that a tool call is required and invoke the tool on the LLM'S behalf the output from the tool will then be sent back to the llm wich will compose its final response for the user.
---------------------------------------------------------------------------------
The output from a tool call is another type of message in the conversation. 
Tool calling steps are typically not shown to the user: the Agent retrieves the conversation, calls the tool(s), gets the outputs, adds them as a new conversation message, and sends the updated conversation to the LLM again.
 From the user‚Äôs point of view, it‚Äôs like the LLM had used the tool, but in fact it was our application code (the Agent) who did it.





<---step by step exsample of how  LLM + Agent + Weather Tool together-->
1Ô∏è‚É£ User asks a question
-------------------------------------------------
user_query = "What's the weather in Paris?"
üí° Explanation: The user sends a question to the LLM.





2Ô∏è‚É£ The LLM Recognizes It Needs a Tool and Generates a Tool Call
üîπ How the LLM Knows About Tools:
-The LLM does not have built-in access to tools.
-It learns about available tools because they are defined in the system_message during chat setup.
-If the user asks about something that matches a tool (e.g., weather), the LLM generates a JSON tool call instead of answering directly.

llm_output = llm_response(user_query)  # LLM generates tool call



 [FUNCTION] LLM Generates Tool Call
 -------------------------------------
def llm_response(user_input):
    if "weather" in user_input.lower():
        # LLM recognizes this as a tool call opportunity
        return json.dumps({"tool": "get_weather", "location": "Paris"})
    else:
        return "I'm not sure how to help with that."





3Ô∏è‚É£ The Agent Detects the Tool Request and Executes It
üîπ The Agent‚Äôs Role:
-The Agent reads the LLM‚Äôs output and determines if a tool needs to be used.
-If a tool call is detected, it executes the correct function.
-The tool processes the request and returns the result.


final_response = agent(llm_output) #Agent processes the tool call


[FUNCTION] Agent Calls the Tool
-------------------------------------
def agent(llm_output):
    try:
        tool_request = json.loads(llm_output)  # Parse LLM's output
        if tool_request.get("tool") == "get_weather":
            location = tool_request.get("location")
            weather_info = get_weather(location)  # Call the weather tool
            return f"The weather in {location} is {weather_info}."
    except json.JSONDecodeError:
        return llm_output  # No tool call detected, return normal response





4Ô∏è‚É£ The Tool Returns Data ‚Üí LLM Formats the Final Response
print(final_response)  # Output the final response
üí° Explanation: The tool fetches real data (e.g., from an API), and the agent returns the final answer to the user.









<----How do we give tools to an LLM----->
we provide the tools in the system prompt with textual descriptions of available tools too the model
-----------------------------------------------------------------------------------------------------------

system_message =""" You are an AI assistant desgined to help users efficiently and accurately. 
your primary goal is to provide helpful, precise and clear responses.
you have access to the following tools: 
{tools_description}
"""

for this too work we have to be precise accurate about
1. what the tool does.
2. What  exact inputs it expects



we will implement a simplified calculator tool that will just multiply two integers
-----------------------------------------------------------------------------------------------------------
def calculat(a: int, b: int) -> int:
     """Multiply two integers."""
    return a * b

info: so our tool is called calculator, it multiplies two integers, and it requires the following inputs:

a(int): An integer 

b(int) An integer

the output of the tool is another integer number that we can describe like this --->
(int) the product of a and b.

All of these details are important. let's put them together in a text string that describes our tool for the LLM to understand --->


Reminder: This textual description is what we want the LLM to know about the tool.
[Textual description]
Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int


When we pass the previous string as part of the input to the LLM, the model will recognize it as a tool, If we want to provide additional tools, we must be consistent and always use the same format. This process can be fragile, and we might accidentally overlook some details

Is there a better way? yes ---> auto formatting tool sections


<---- Auto-formatting Tool sections---->
-------------------------------------------------------------------

